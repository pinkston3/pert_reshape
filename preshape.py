import argparse
import multiprocessing
import os
import re
import sys
import time

from typing import Tuple

import h5py
import progressbar


DEFAULT_FROMDIR = './tmp'
FILE_REGEX = re.compile(r'([^_]+)_eph_g2_p(\d+)\.h5')

DEFAULT_MAX_PROCESSES = 50
SUBPROCESS_REPORT_INTERVAL = 3.0 # in seconds


def make_pool_filename(prefix: str, pool: int) -> str:
    '''
    Generate a pool filename from a prefix and pool number.
    '''
    assert len(prefix) > 0, 'prefix must be specified'
    assert pool > 0, f'pool must be positive; got {pool}'
    return f'{prefix}_eph_g2_p{pool}.h5'


def kloc_index_to_pool_index(kloc, npools) -> Tuple[int, int]:
    '''
    Map a k-location index to a pool index and an index within the pool.  All
    indexes are zero-based throughout this program, but the HDF5 files' names
    and data key-names are all 1-based since they are used from Fortran.
    '''
    assert kloc >= 0, f'k-grid location must be nonnegative; got {kloc}'
    assert npools >= 1, f'Must have at least 1 pool; got {npools}'
    return (kloc % npools, kloc // npools)


class PoolFile:
    '''
    A single HDF5 file in the set of pool data files.
    '''

    def __init__(self, filename: str, pool: int):
        self.filename = filename
        self.pool = pool
        self.hdf5 = None

        # These values are generated by scanning the file
        self.nk_loc = 0
        self.nkq = 0

    def __repr__(self):
        return self.filename

    def open(self, mode):
        '''
        Open the pool's HDF5 file with the specified mode, e.g. 'r' or 'w'.
        This operation is called by other methods on the class.

        An assertion will fail if the file is opened more than once, without
        closing it in between.
        '''
        assert self.hdf5 is None, f'HDF5 file {self.filename} is already open'
        self.hdf5 = h5py.File(self.filename, mode)

    def close(self):
        '''
        Close the pool's HDF5 file.
        '''
        if self.hdf5 is None:
            return

        self.hdf5.close()
        self.hdf5 = None

    def scan_contents(self):
        self.open('r')
        self.nk_loc = 0
        self.nkq = 0
        while True:
            i = self.nk_loc + 1
            bnd_idx = f'bands_index_{i}'
            eph_g2 = f'eph_g2_{i}'

            if (bnd_idx not in self.hdf5 and eph_g2 not in self.hdf5):
                break

            if ((bnd_idx in self.hdf5 and not eph_g2 in self.hdf5) or
                (bnd_idx not in self.hdf5 and eph_g2 in self.hdf5)):
                raise ValueError(f'Only found one of {bnd_idx} / {eph_g2} in file {self.filename}')

            if len(self.hdf5[bnd_idx]) != len(self.hdf5[eph_g2]):
                raise ValueError(f'Lengths of {bnd_idx} and {eph_g2} differ in file {self.filename}')

            # If we got here then there are no obvious issues with this pair
            # of entries
            self.nk_loc += 1
            self.nkq += len(self.hdf5[bnd_idx])

    def get_eph_g2(self, index):
        '''
        Return the eph_g2 dataset for the specified index.  The dataset's
        name will be "eph_g2_{index}" in the HDF5 file.
        '''
        return self.hdf5[f'eph_g2_{index}']

    def get_bands_index(self, index):
        '''
        Return the bands_index dataset for the specified index.  The
        dataset's name will be "bands_index_{index}" in the HDF5 file.
        '''
        return self.hdf5[f'bands_index_{index}']

    def make_new(self):
        self.open('w')

    def set_eph_g2(self, index, data):
        '''
        Create an eph_g2 dataset for the specified index.  The dataset's
        name will be "eph_g2_{index}" in the HDF5 file.
        '''
        return self.hdf5.create_dataset(f'eph_g2_{index}', data=data)

    def set_bands_index(self, index, data):
        '''
        Create a bands_index dataset for the specified index.  The
        dataset's name will be "bands_index_{index}" in the HDF5 file.
        '''
        return self.hdf5.create_dataset(f'bands_index_{index}', data=data)


# Define the function that runs in the subprocess
def mp_scan_perturbo_hdf5_file(filename, pool):
    '''
    This is the subprocess function that scans a Perturbo pool HDF5 file to
    verify that everything looks correct, and to determine some essential
    details of the pool file.

    The process receives a pipe ``conn`` from the parent process, to which
    it sends back the number of k-grid locations, and the number of k-q
    pairs, in the file.

    NOTE:  It seems like this needs to be a top-level function so it can
           be pickled and passed to the subprocess.  There may be a better
           way to do this in the long run, but this'll do for now.
    '''
    f = PoolFile(filename, pool)
    f.scan_contents()
    return (f.nk_loc, f.nkq)


class PoolFileSet:
    '''
    A collection of pool data files that drive a Perturbo simulation run.
    '''

    def __init__(self, path):
        self.path = path
        self.num_pools = 0
        self.pool_files = {}
        self.prefix = None
        self.nkpt = 0
        self.nkq = 0

    def find_files(self):
        '''
        Scan the file-set's path for pool files.  This is used for the source
        set of data files, but isn't used when generating a new set of data
        files.
        '''
        self.num_pools = 0
        self.pool_files = {}
        self.prefix = None

        files = os.listdir(self.path)
        for filename in files:
            match = FILE_REGEX.fullmatch(filename)
            if match:
                prefix = match.group(1)
                pool = int(match.group(2))

                if self.prefix is None:
                    self.prefix = prefix
                else:
                    if self.prefix != prefix:
                        raise ValueError(f'File {filename} doesn\'t have expected prefix {self.prefix}')

                if pool in self.pool_files:
                    raise ValueError(f'Pool {pool} appears in multiple filenames')

                self.num_pools += 1
                self.pool_files[pool] = PoolFile(os.path.join(self.path, filename), pool)

        for i in range(1, self.num_pools + 1):
            if i not in self.pool_files:
                raise ValueError(f'Can\'t find file for pool {i} ,in {self.num_pools} pools')

    def scan_files(self, progress=None):
        '''
        Open each HDF5 pool data file found by the ``find_files`` method,
        and scan its contents to see if they make sense, and to see how many
        k-grid locations are in each file.

        Since this is a slow operation, callers can optionally provide a
        ``progress(pool: int, f: PoolFile)`` callback function; this function
        is called after the file ``f`` has been scanned by the operation.
        Files are scanned, and will be reported to the progress function, in
        order of increasing pool-number.
        '''
        self.nkpt = 0
        self.nkq = 0
        for pool in sorted(self.pool_files.keys()):
            f = self.pool_files[pool]
            f.scan_contents()
            self.nkpt += f.nk_loc
            self.nkq += f.nkq

            if progress:
                progress(pool, f)

    def scan_files_mp(self, progress=None, **kwargs):
        '''
        Open each HDF5 pool data file found by the ``find_files`` method,
        and scan its contents to see if they make sense, and to see how many
        k-grid locations are in each file.

        This version differs from ``scan_files()`` in that it uses the Python
        ``multiprocessing`` library to scan all files concurrently, using one
        subprocess per file to scan.  Since scanning the source files is an
        IO-intensive operation, parallelizing it will almost certainly result
        in significant performance improvements.

        Since this is a slow operation, callers can optionally provide a
        ``progress(pool: int, f: PoolFile)`` callback function; this function
        is called after the file ``f`` has been scanned by the operation.
        Because of the use of multiple process, files are scanned in no
        specific order, but they are still reported to the progress function
        in order of increasing pool-number.
        '''

        max_processes = kwargs.get('max_processes', DEFAULT_MAX_PROCESSES)

        self.nkpt = 0
        self.nkq = 0

        # Spin up a subprocess for each input file to scan.  This way we can
        # scan them concurrently.

        # Unfortunately we have "multiprocessing.Pool" and also the
        # Perturbo pool files...
        exec_pool = multiprocessing.Pool(max_processes)
        results: list[tuple[PoolFile, multiprocessing.pool.AsyncResult]] = []

        # Queue up the scans of all the pool files for execution.
        for pool in sorted(self.pool_files.keys()):
            f = self.pool_files[pool]
            r = exec_pool.apply_async(mp_scan_perturbo_hdf5_file, (f.filename, pool) )
            results.append( (f, r) )

        exec_pool.close()

        # Wait for results to come back in order so our output looks nice.

        errors = 0
        for (f, r) in results:
            try:
                value = r.get()

                # Seems like things worked - unpack the result
                (nk_loc, nkq) = value
                f.nk_loc = nk_loc
                f.nkq = nkq
                self.nkpt += nk_loc
                self.nkq += nkq

                if progress:
                    progress(pool, f)

            except BaseException as err:
                print(f'ERROR:  exception while scanning pool-file {f.pool}:')
                traceback.print_exception(err)
                errors += 1

        exec_pool.join()

        if not errors:
            self.open_all('r')
        else:
            raise RuntimeError(f'{errors} error(s) detected during pool-file scans')


    def make_new_pool_files(self, prefix, num_pools):
        '''
        Generate a set of new pool data files with the specified file-prefix.
        An HDF5 file is generated in the target directory for each pool, so
        that they can begin being populated by the reshaping process.

        This object must not already have a set of pool files, or an error
        will be reported.

        The number of pools specified must be at least 1.
        '''
        assert len(prefix) > 0, 'Must specify a file-name prefix'
        assert num_pools > 0, f'Must specify at least 1 pool; got {num_pools}'
        assert self.num_pools == 0
        assert len(self.pool_files) == 0

        self.prefix = prefix
        self.num_pools = num_pools

        for pool in range(1, self.num_pools + 1):
            filename = make_pool_filename(self.prefix, pool)
            f = PoolFile(os.path.join(self.path, filename), pool)
            f.make_new()
            self.pool_files[pool] = f

    def open_all(self, mode):
        for f in self.pool_files.values():
            f.open(mode)

    def close_all(self):
        for f in self.pool_files.values():
            f.close()


def parse_args():
    parser = argparse.ArgumentParser(prog='preshape',
        description='Reshape Perturbo tmp/ files for a different number of pools')

    parser.add_argument('-f', '--fromdir', default=DEFAULT_FROMDIR,
        help=f'Source directory to read eph_g2_p*.h5 files from.  Default is {DEFAULT_FROMDIR}.')

    parser.add_argument('-t', '--todir', required=True,
        help='Target directory to write reshaped eph_g2_p*.h5 files to.')

    parser.add_argument('-p', '--pools', type=int, required=True,
        help='Number of pools to generate in the target directory.')

    parser.add_argument('-n', '--dryrun', action='store_true',
        help='Perform a dry-run; don\'t write any target files out.')

    parser.add_argument('-q', '--quiet', action='store_true',
        help='Run in "quiet mode," with a minimum of output.')

    parser.add_argument('--mp', action='store_true',
        help='Use multiprocessing to speed up reshape operations.')

    parser.add_argument('-M', '--max-processes', type=int, default=DEFAULT_MAX_PROCESSES,
        help=f'Specify maximum number of subprocesses to use.  Default is {DEFAULT_MAX_PROCESSES}.')

    return parser.parse_args()


def check_args(args):
    # Check arguments

    print(f'Reading pool files from {args.fromdir}')
    if not os.path.isdir(args.fromdir):
        print(f'ERROR:  {args.fromdir} is not a directory')
        sys.exit(1)

    if args.pools < 1:
        print(f'ERROR:  Number of pools must be positive; got {args.pools}')
        sys.exit(1)

    print(f'Writing {args.pools} pool files to {args.todir}')
    if os.path.exists(args.todir):
        existing_files = os.listdir(args.todir)
        if len(existing_files) > 0:
            print(f'ERROR:  Existing files found in {args.todir}, aborting.')
            sys.exit(1)

    if args.mp:
        print(f'\nUsing multiprocessing to speed up performance.  Max processes = {args.max_processes}.')

def file_scan_progress(pool, f, max_filename_len):
    # This is grungy because we want to add extra string padding after the
    # ':' character, not before it.
    out_filename = f.filename + ':  '
    if len(out_filename) < max_filename_len + 3:
        out_filename = out_filename + ' ' * (max_filename_len + 3 - len(out_filename))

    print(f' * {out_filename}nk_loc = {f.nk_loc}\tnkq = {f.nkq}')

def scan_source_directory(args):
    print(f'\nScanning source directory {args.fromdir}')

    sfset = PoolFileSet(args.fromdir)
    sfset.find_files()
    if sfset.num_pools == 0:
        print('ERROR:  Found no pool data files in source directory, aborting.')
        sys.exit(1)

    max_filename_len = max([len(f.filename) for f in sfset.pool_files.values()])

    if not args.quiet:
        print(f'Found {sfset.num_pools} files:')

    progress = None
    if not args.quiet:
        progress=lambda pool, f : file_scan_progress(pool, f, max_filename_len)

    if args.mp:
        sfset.scan_files_mp(progress=progress, max_processes=args.max_processes)
    else:
        sfset.scan_files(progress=progress)

    if not args.quiet:
        print(f'Total k-grid points:  {sfset.nkpt}\tTotal k-q pairs:  {sfset.nkq}')

    return sfset


def write_new_target_files(args, sfset):
    print(f'\nWriting new set of pool files to directory {args.todir}')

    if not os.path.exists(args.todir):
        print(f'NOTE:  {args.todir} doesn\'t exist; creating')
        os.makedirs(args.todir)

    tfset = PoolFileSet(args.todir)
    tfset.make_new_pool_files(sfset.prefix, args.pools)

    bar = progressbar.ProgressBar(max_value=sfset.nkpt)
    bar.start()
    for i_kloc in range(sfset.nkpt):
        (src_pool, src_idx) = kloc_index_to_pool_index(i_kloc, sfset.num_pools)
        (tgt_pool, tgt_idx) = kloc_index_to_pool_index(i_kloc, tfset.num_pools)

        src_f = sfset.pool_files[src_pool + 1]
        tgt_f = tfset.pool_files[tgt_pool + 1]

        tgt_f.set_eph_g2(tgt_idx + 1, src_f.get_eph_g2(src_idx + 1))
        tgt_f.set_bands_index(tgt_idx + 1, src_f.get_bands_index(src_idx + 1))

        bar.update(i_kloc + 1)
    bar.finish()
    tfset.close_all()
    return tfset


def mp_generate_perturbo_hdf5_file(filename, pool, num_pools, sfset, queue):
    sfset.open_all('r')

    tgt_f = PoolFile(filename, pool + 1)
    tgt_f.open('w')
    tgt_f.nk_loc = 0

    count = 0
    t = time.time()
    for i_kloc in range(pool, sfset.nkpt, num_pools):
        (src_pool, src_idx) = kloc_index_to_pool_index(i_kloc, sfset.num_pools)
        (tgt_pool, tgt_idx) = kloc_index_to_pool_index(i_kloc, num_pools)

        # All of the k-grid indexes we are traversing should map to this
        # specific target file.
        assert tgt_pool == pool, f'k-grid index {i_kloc} maps to {tgt_pool}, not expected pool {pool}'

        src_f = sfset.pool_files[src_pool + 1]

        tgt_f.set_eph_g2(tgt_idx + 1, src_f.get_eph_g2(src_idx + 1))
        tgt_f.set_bands_index(tgt_idx + 1, src_f.get_bands_index(src_idx + 1))

        tgt_f.nk_loc += 1

        count += 1
        t2 = time.time()
        if (t2 - t) >= SUBPROCESS_REPORT_INTERVAL:
            queue.put( (pool, count) )
            count = 0
            t = t2

    queue.put( (pool, count) )
    queue.put( (pool, None) )
    queue.close()


def mp_write_new_target_files(args, sfset, **kwargs):
    print(f'\nWriting new set of pool files to directory {args.todir}')

    if not os.path.exists(args.todir):
        print(f'NOTE:  {args.todir} doesn\'t exist; creating')
        os.makedirs(args.todir)

    max_processes = kwargs.get('max_processes', DEFAULT_MAX_PROCESSES)

    # Since we pass the source fileset to the subprocesses, we need to close
    # the HDF5 files since we can't pickle them.
    sfset.close_all()

    exec_pool = multiprocessing.Pool(max_processes)
    tasks = {}
    manager = multiprocessing.Manager()
    queue = manager.Queue()

    # Queue up a task for each target file we are writing.
    for tgt_pool in range(args.pools):
        tgt_filename = make_pool_filename(sfset.prefix, tgt_pool + 1)
        tgt_filename = os.path.join(args.todir, tgt_filename)

        r = exec_pool.apply_async(mp_generate_perturbo_hdf5_file,
            (tgt_filename, tgt_pool, args.pools, sfset, queue))

        tasks[tgt_pool] = r

    exec_pool.close()

    # Monitor the subprocesses for their completion.

    if not args.quiet:
        bar = progressbar.ProgressBar(max_value=sfset.nkpt)
        bar.start()

    count = 0
    while len(tasks) > 0:
        (tgt_pool, value) = queue.get()

        if value is None:
            # Finished processing specified pool.
            # TODO:  Not sure if this is necessary.  tasks[tgt_pool].get()
            del tasks[tgt_pool]
        else:
            count += value

            if not args.quiet:
                bar.update(count)

    if not args.quiet:
        bar.finish()

    # Clean up the executor pool.
    exec_pool.join()


def main():
    args = parse_args()
    check_args(args)

    sfset = scan_source_directory(args)

    if not args.dryrun:
        if args.mp:
            mp_write_new_target_files(args, sfset, max_processes=args.max_processes)
        else:
            write_new_target_files(args, sfset)
    else:
        print('\nDry-run requested, not writing output files.')

    sfset.close_all()

    print('\nDone!')
    sys.exit(0)


if __name__ == '__main__':
    main()
