import argparse
import multiprocessing
import os
import re
import sys

from typing import Tuple

import h5py
import progressbar


DEFAULT_FROMDIR = './tmp'
FILE_REGEX = re.compile(r'([^_]+)_eph_g2_p(\d+)\.h5')


def make_pool_filename(prefix: str, pool: int) -> str:
    '''
    Generate a pool filename from a prefix and pool number.
    '''
    return f'{prefix}_eph_g2_p{pool}.h5'


def kloc_index_to_pool_index(kloc, npools) -> Tuple[int, int]:
    '''
    Map a k-location index to a pool index and an index within the pool.  All
    indexes are zero-based throughout this program, but the HDF5 files' names
    and data key-names are all 1-based since they are used from Fortran.
    '''
    assert npools >= 1, f'Must have at least 1 pool; got {npools}'
    return (kloc % npools, kloc // npools)


class PoolFile:
    '''
    A single HDF5 file in the set of pool data files.
    '''

    def __init__(self, filename: str, pool: int):
        self.filename = filename
        self.pool = pool
        self.hdf5 = None

        # These values are generated by scanning the file
        self.nk_loc = 0
        self.nkq = 0

    def __repr__(self):
        return self.filename

    def open(self, mode):
        '''
        Open the pool's HDF5 file with the specified mode, e.g. 'r' or 'w'.
        This operation is called by other methods on the class.

        An assertion will fail if the file is opened more than once, without
        closing it in between.
        '''
        assert self.hdf5 is None, f'HDF5 file {self.filename} is already open'
        self.hdf5 = h5py.File(self.filename, mode)

    def close(self):
        '''
        Close the pool's HDF5 file.
        '''
        if self.hdf5 is None:
            return

        self.hdf5.close()
        self.hdf5 = None

    def scan_contents(self):
        self.open('r')
        self.nk_loc = 0
        self.nkq = 0
        while True:
            i = self.nk_loc + 1
            bnd_idx = f'bands_index_{i}'
            eph_g2 = f'eph_g2_{i}'

            if (bnd_idx not in self.hdf5 and eph_g2 not in self.hdf5):
                break

            if ((bnd_idx in self.hdf5 and not eph_g2 in self.hdf5) or
                (bnd_idx not in self.hdf5 and eph_g2 in self.hdf5)):
                raise ValueError(f'Only found one of {bnd_idx} / {eph_g2} in file {self.filename}')

            if len(self.hdf5[bnd_idx]) != len(self.hdf5[eph_g2]):
                raise ValueError(f'Lengths of {bnd_idx} and {eph_g2} differ in file {self.filename}')

            # If we got here then there are no obvious issues with this pair
            # of entries
            self.nk_loc += 1
            self.nkq += len(self.hdf5[bnd_idx])

    def get_eph_g2(self, index):
        return self.hdf5[f'eph_g2_{index}']

    def get_bands_index(self, index):
        return self.hdf5[f'bands_index_{index}']

    def make_new(self):
        self.open('w')

    def set_eph_g2(self, index, data):
        return self.hdf5.create_dataset(f'eph_g2_{index}', data=data)

    def set_bands_index(self, index, data):
        return self.hdf5.create_dataset(f'bands_index_{index}', data=data)


# Define the function that runs in the subprocess
#
# NOTE(donnie):  Seems like this needs to be a top-level function so it can
#     be pickled and passed to the subprocess.
def mp_scan_perturbo_hdf5_file(filename, pool, conn):
    try:
        f = PoolFile(filename, pool)
        f.scan_contents()
        conn.send( (f.nk_loc, f.nkq) )
    except BaseException as err:
        conn.send(err)

    conn.close()


class PoolFileSet:
    '''
    A collection of pool data files that drive a Perturbo simulation run.
    '''

    def __init__(self, path):
        self.path = path
        self.num_pools = 0
        self.pool_files = {}
        self.prefix = None
        self.nkpt = 0

    def find_files(self):
        '''
        Scan the file-set's path for pool files.  This is used for the source
        set of data files, but isn't used when generating a new set of data
        files.
        '''
        self.num_pools = 0
        self.pool_files = {}
        self.prefix = None

        files = os.listdir(self.path)
        for filename in files:
            match = FILE_REGEX.fullmatch(filename)
            if match:
                prefix = match.group(1)
                pool = int(match.group(2))

                if self.prefix is None:
                    self.prefix = prefix
                else:
                    if self.prefix != prefix:
                        raise ValueError(f'File {filename} doesn\'t have expected prefix {self.prefix}')

                if pool in self.pool_files:
                    raise ValueError(f'Pool {pool} appears in multiple filenames')

                self.num_pools += 1
                self.pool_files[pool] = PoolFile(os.path.join(self.path, filename), pool)

        for i in range(1, self.num_pools + 1):
            if i not in self.pool_files:
                raise ValueError(f'Can\'t find file for pool {i} ,in {self.num_pools} pools')

    def scan_files(self, progress=None):
        '''
        Open each HDF5 pool data file found by the ``find_files`` method,
        and scan its contents to see if they make sense, and to see how many
        k-grid locations are in each file.

        Since this is a slow operation, callers can optionally provide a
        ``progress(pool: int, f: PoolFile)`` callback function; this function
        is called after the file ``f`` has been scanned by the operation.
        '''
        self.nkpt = 0
        for pool in sorted(self.pool_files.keys()):
            f = self.pool_files[pool]
            f.scan_contents()
            self.nkpt += f.nk_loc

            if progress:
                progress(pool, f)


    def scan_files_mp(self, progress=None):
        '''
        Open each HDF5 pool data file found by the ``find_files`` method,
        and scan its contents to see if they make sense, and to see how many
        k-grid locations are in each file.

        Since this is a slow operation, callers can optionally provide a
        ``progress(pool: int, f: PoolFile)`` callback function; this function
        is called after the file ``f`` has been scanned by the operation.
        '''

        self.nkpt = 0

        # Spin up a subprocess for each input file to scan.  This way we can
        # scan them concurrently.
        subprocesses = []
        for pool in sorted(self.pool_files.keys()):
            f = self.pool_files[pool]

            (parent_conn, child_conn) = multiprocessing.Pipe()
            proc = multiprocessing.Process(target=mp_scan_perturbo_hdf5_file,
                args=(f.filename, pool, child_conn))
            proc.start()
            subprocesses.append((f, proc, parent_conn))

        for (f, proc, parent_conn) in subprocesses:
            pool = f.pool
            value = parent_conn.recv()
            proc.join()
            if isinstance(value, BaseException):
                raise value

            # Seems like things worked - unpack the result
            (nk_loc, nkq) = value
            f.nk_loc = nk_loc
            f.nkq = nkq
            self.nkpt += nk_loc

            # Also need to open the file; this process hasn't opened it yet
            f.open('r')

            if progress:
                progress(pool, f)


    def make_new_pool_files(self, prefix, num_pools):
        '''
        Generate a set of new pool data files with the specified file-prefix.
        An HDF5 file is generated in the target directory for each pool, so
        that they can begin being populated by the reshaping process.

        This object must not already have a set of pool files, or an error
        will be reported.

        The number of pools specified must be at least 1.
        '''
        assert len(prefix) > 0, 'Must specify a file-name prefix'
        assert num_pools > 0, f'Must specify at least 1 pool; got {num_pools}'
        assert self.num_pools == 0
        assert len(self.pool_files) == 0

        self.prefix = prefix
        self.num_pools = num_pools

        for pool in range(1, self.num_pools + 1):
            filename = make_pool_filename(self.prefix, pool)
            f = PoolFile(os.path.join(self.path, filename), pool)
            f.make_new()
            self.pool_files[pool] = f

    def open_all(self, mode):
        for f in self.pool_files.values():
            f.open(mode)

    def close_all(self):
        for f in self.pool_files.values():
            f.close()


def parse_args():
    parser = argparse.ArgumentParser(prog='preshape',
        description='Reshape Perturbo tmp/ files for a different number of pools')

    parser.add_argument('-f', '--fromdir', default=DEFAULT_FROMDIR,
        help=f'Source directory to read eph_g2_p*.h5 files from.  Default is {DEFAULT_FROMDIR}.')

    parser.add_argument('-t', '--todir', required=True,
        help='Target directory to write reshaped eph_g2_p*.h5 files to.')

    parser.add_argument('-p', '--pools', type=int, required=True,
        help='Number of pools to generate in the target directory.')

    parser.add_argument('--mp', action='store_true',
        help='Use multiprocessing to speed up reshape operations.')

    args = parser.parse_args(sys.argv[1:])
    return args

def check_args(args):
    # Check arguments

    print(f'Reading pool files from {args.fromdir}')
    if not os.path.isdir(args.fromdir):
        print(f'ERROR:  {args.fromdir} is not a directory')
        sys.exit(1)

    if args.pools < 1:
        print(f'ERROR:  Number of pools must be positive; got {args.pools}')
        sys.exit(1)

    print(f'Writing {args.pools} pool files to {args.todir}')
    if not os.path.exists(args.todir):
        print(f'NOTE:  {args.todir} doesn\'t exist; creating')
        os.makedirs(args.todir)
    else:
        existing_files = os.listdir(args.todir)
        if len(existing_files) > 0:
            print(f'ERROR:  Existing files found in {args.todir}, aborting.')
            sys.exit(1)

    if args.mp:
        print(f'\nUsing multiprocessing to speed up performance.')


def scan_source_directory(args):
    print(f'\nScanning source directory {args.fromdir}')

    sfset = PoolFileSet(args.fromdir)
    sfset.find_files()
    if sfset.num_pools == 0:
        print('ERROR:  Found no pool data files in source directory, aborting.')
        sys.exit(1)

    print(f'Found {sfset.num_pools} files:')
    scan_fn = sfset.scan_files
    if args.mp:
        scan_fn = sfset.scan_files_mp

    scan_fn(progress=lambda pool, f : print(f' * {f.filename}:\tnk_loc = {f.nk_loc}\tnkq = {f.nkq}'))

    print(f'Total k-grid points found:  {sfset.nkpt}')

    return sfset


def write_new_target_files(args, sfset):
    print(f'\nWriting new set of pool files to directory {args.todir}')

    tfset = PoolFileSet(args.todir)
    tfset.make_new_pool_files(sfset.prefix, args.pools)

    bar = progressbar.ProgressBar(max_value=sfset.nkpt)
    bar.start()
    for i_kloc in range(sfset.nkpt):
        (src_pool, src_idx) = kloc_index_to_pool_index(i_kloc, sfset.num_pools)
        (tgt_pool, tgt_idx) = kloc_index_to_pool_index(i_kloc, tfset.num_pools)

        src_f = sfset.pool_files[src_pool + 1]
        tgt_f = tfset.pool_files[tgt_pool + 1]

        tgt_f.set_eph_g2(tgt_idx + 1, src_f.get_eph_g2(src_idx + 1))
        tgt_f.set_bands_index(tgt_idx + 1, src_f.get_bands_index(src_idx + 1))

        bar.update(i_kloc + 1)
    bar.finish()
    tfset.close_all()
    return tfset


def mp_generate_perturbo_hdf5_file(filename, pool, num_pools, sfset, conn):
    try:
        sfset.open_all('r')

        tgt_f = PoolFile(filename, pool + 1)
        tgt_f.open('w')
        tgt_f.nk_loc = 0

        for i_kloc in range(pool, sfset.nkpt, num_pools):
            (src_pool, src_idx) = kloc_index_to_pool_index(i_kloc, sfset.num_pools)
            (tgt_pool, tgt_idx) = kloc_index_to_pool_index(i_kloc, num_pools)

            # All of the k-grid indexes we are traversing should map to this
            # specific target file.
            assert tgt_pool == pool, f'k-grid index {i_kloc} maps to {tgt_pool}, not expected pool {pool}'

            src_f = sfset.pool_files[src_pool + 1]

            tgt_f.set_eph_g2(tgt_idx + 1, src_f.get_eph_g2(src_idx + 1))
            tgt_f.set_bands_index(tgt_idx + 1, src_f.get_bands_index(src_idx + 1))

            tgt_f.nk_loc += 1

        conn.send(tgt_f.nk_loc)

    except BaseException as err:
        conn.send(err)

    conn.close()


def mp_write_new_target_files(args, sfset):
    print(f'\nWriting new set of pool files to directory {args.todir}')

    # Since we pass the source fileset to the subprocesses, we need to close
    # the HDF5 files since we can't pickle them.
    sfset.close_all()

    # Spin up a subprocess for each target file we are writing.
    subprocesses = []
    for pool in range(args.pools):
        filename = make_pool_filename(sfset.prefix, pool + 1)
        filename = os.path.join(args.todir, filename)
        (parent_conn, child_conn) = multiprocessing.Pipe()

        proc = multiprocessing.Process(target=mp_generate_perturbo_hdf5_file,
            args=(filename, pool, args.pools, sfset, child_conn))
        subprocesses.append((pool, proc, parent_conn))
        proc.start()

    # Monitor the subprocesses for their completion.
    for (pool, proc, parent_conn) in subprocesses:
        value = parent_conn.recv()
        proc.join()
        if isinstance(value, BaseException):
            raise value


def main():
    args = parse_args()
    check_args(args)

    sfset = scan_source_directory(args)

    if args.mp:
        mp_write_new_target_files(args, sfset)
    else:
        write_new_target_files(args, sfset)

    sfset.close_all()

    print('\nDone!')
    sys.exit(0)


if __name__ == '__main__':
    main()
